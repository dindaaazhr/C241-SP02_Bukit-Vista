{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HbJzuys_qVzI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "cN0nYDD-PycE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlxhVRPZPzRJ",
        "outputId": "6532e33f-6e05-40bd-b16d-e4d688e69f7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = \"drive/MyDrive/dataset/\""
      ],
      "metadata": {
        "id": "fPIyLpg3P0_N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.read_csv(main_dir + 'final_data_new_regency.csv')\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "b_CFxr39P4Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "metadata": {
        "id": "S1VFjjxJnnXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering DBScan"
      ],
      "metadata": {
        "id": "8UBHiPOGUwwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Assuming `final_df` is your dataset with appropriate columns\n",
        "\n",
        "# Define the columns that need to be scaled and encoded\n",
        "num_cols = ['total_guest_capacity', 'avg_price_per_day', 'area_distance_to_airport']\n",
        "cat_cols = ['property_type', 'area_regency_city']\n",
        "\n",
        "# Create the transformers\n",
        "scaler = MinMaxScaler()\n",
        "encoder = OneHotEncoder(drop='first')\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, num_cols),\n",
        "        ('cat', encoder, cat_cols)\n",
        "    ])\n",
        "\n",
        "# Define the KMeans clustering algorithm with the new parameters\n",
        "kmeans = KMeans(n_clusters=8, init='random', n_init=50, max_iter=300, tol=1e-04, random_state=0)\n",
        "\n",
        "# Define the DBSCAN clustering algorithm\n",
        "dbscan = DBSCAN(eps=1.15, min_samples=3)\n",
        "\n",
        "# Define the AgglomerativeClustering algorithm\n",
        "agglo = AgglomerativeClustering(n_clusters=19, linkage='single')\n",
        "\n",
        "# Create pipelines for each clustering algorithm\n",
        "pipeline_kmeans = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', kmeans)])\n",
        "pipeline_dbscan = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', dbscan)])\n",
        "pipeline_agglo = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', agglo)])\n",
        "\n",
        "# Fit the pipelines\n",
        "pipeline_kmeans.fit(final_df)\n",
        "pipeline_dbscan.fit(final_df)\n",
        "pipeline_agglo.fit(final_df)\n",
        "\n",
        "# Get cluster labels from each algorithm\n",
        "labels_kmeans = pipeline_kmeans.named_steps['cluster'].labels_\n",
        "labels_dbscan = pipeline_dbscan.named_steps['cluster'].labels_\n",
        "labels_agglo = pipeline_agglo.named_steps['cluster'].labels_\n",
        "\n",
        "# If DBSCAN results in a single cluster, modify eps or min_samples\n",
        "if len(set(labels_dbscan)) <= 1:\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    pipeline_dbscan = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', dbscan)])\n",
        "    pipeline_dbscan.fit(final_df)\n",
        "    labels_dbscan = pipeline_dbscan.named_steps['cluster'].labels_\n",
        "\n",
        "# Gather clustering results\n",
        "clusterings = [labels_kmeans, labels_dbscan, labels_agglo]\n",
        "\n",
        "# Create the co-association matrix\n",
        "n_samples = final_df.shape[0]\n",
        "co_assoc_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "for labels in clusterings:\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                co_assoc_matrix[i, j] += 1\n",
        "\n",
        "# Normalize the co-association matrix\n",
        "co_assoc_matrix /= len(clusterings)\n",
        "\n",
        "# Perform clustering on the co-association matrix\n",
        "distance_matrix = 1 - co_assoc_matrix\n",
        "hierarchical_clustering = linkage(squareform(distance_matrix), method='average')\n",
        "ensemble_labels = fcluster(hierarchical_clustering, t=8, criterion='maxclust')\n",
        "\n",
        "# Add ensemble labels to the dataframe\n",
        "final_df['ensemble_cluster_label'] = ensemble_labels\n",
        "\n",
        "# Preprocess the data for evaluation metrics\n",
        "preprocessed_data = preprocessor.fit_transform(final_df)\n",
        "\n",
        "# Check the number of unique clusters in the ensemble result\n",
        "unique_clusters = len(set(ensemble_labels))\n",
        "if unique_clusters > 1:\n",
        "    # Evaluate the ensemble clustering result\n",
        "    silhouette_avg = silhouette_score(preprocessed_data, ensemble_labels)\n",
        "    davies_bouldin_avg = davies_bouldin_score(preprocessed_data, ensemble_labels)\n",
        "    print('Ensemble Silhouette Score:', silhouette_avg)\n",
        "    print('Ensemble Davies-Bouldin Index:', davies_bouldin_avg)\n",
        "else:\n",
        "    print(f\"Ensemble clustering resulted in {unique_clusters} unique cluster(s). Silhouette score and Davies-Bouldin Index cannot be computed.\")\n",
        "\n",
        "# Print the dataframe with cluster labels\n",
        "print(final_df[['unit_id', 'ensemble_cluster_label']])\n"
      ],
      "metadata": {
        "id": "AKXNq0IjM2k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_avg = silhouette_score(preprocessed_data, ensemble_labels)\n",
        "davies_bouldin_avg = davies_bouldin_score(preprocessed_data, ensemble_labels)\n",
        "print('Ensemble Silhouette Score:', silhouette_avg)\n",
        "print('Ensemble Davies-Bouldin Index:', davies_bouldin_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86vLtgHv0rGM",
        "outputId": "d7c9eb4e-06b9-43e4-9e93-cac164febe27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Silhouette Score: 0.7845887998465851\n",
            "Ensemble Davies-Bouldin Index: 0.8429919540465185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "final_df.sort_values(by='ensemble_cluster_label')"
      ],
      "metadata": {
        "id": "BO97dQSJgPNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the columns that need to be scaled and encoded\n",
        "num_cols = ['total_guest_capacity', 'bedroom', 'bathroom', 'beds', 'avg_price_per_day', 'area_distance_to_airport']\n",
        "cat_cols = ['property_type', 'area_regency_city']\n",
        "\n",
        "# Create the transformers\n",
        "scaler = MinMaxScaler()\n",
        "encoder = OneHotEncoder(drop='first')\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, num_cols),\n",
        "        ('cat', encoder, cat_cols)\n",
        "    ])\n",
        "\n",
        "eps_value = 1.15\n",
        "min_samples_value = 3\n",
        "# Define the DBSCAN clustering algorithm\n",
        "cluster = DBSCAN(eps=eps_value, min_samples=min_samples_value)  # You may need to tune these parameters\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('cluster', cluster)])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(final_df)\n",
        "\n",
        "# Get cluster labels and add them to the DataFrame\n",
        "cluster_labels = pipeline.named_steps['cluster'].labels_\n",
        "final_df['cluster_label'] = cluster_labels\n",
        "\n",
        "print(final_df[['unit_id', 'cluster_label']])"
      ],
      "metadata": {
        "id": "pHlsvXB4cair"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from scipy.cluster.hierarchy import linkage, fcluster\n",
        "# from scipy.spatial.distance import squareform\n",
        "# from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# # Assuming `final_df` is your dataset with appropriate columns\n",
        "\n",
        "# # Define the columns that need to be scaled and encoded\n",
        "# num_cols = ['total_guest_capacity', 'avg_price_per_day', 'area_distance_to_airport']\n",
        "# cat_cols = ['property_type', 'area_regency_city']\n",
        "\n",
        "# # Create the transformers\n",
        "# scaler = MinMaxScaler()\n",
        "# encoder = OneHotEncoder(drop='first')\n",
        "\n",
        "# # Create the column transformer\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('num', scaler, num_cols),\n",
        "#         ('cat', encoder, cat_cols)\n",
        "#     ])\n",
        "\n",
        "# # Define the clustering algorithms with optimized parameters\n",
        "# kmeans = KMeans(n_clusters=8, init='random', n_init=50, max_iter=300, tol=1e-04, random_state=0)\n",
        "# dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "# agglo = AgglomerativeClustering(n_clusters=8, linkage='average')\n",
        "\n",
        "# # Create pipelines for each clustering algorithm\n",
        "# pipeline_kmeans = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', kmeans)])\n",
        "# pipeline_dbscan = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', dbscan)])\n",
        "# pipeline_agglo = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', agglo)])\n",
        "\n",
        "# # Fit the pipelines\n",
        "# pipeline_kmeans.fit(final_df)\n",
        "# pipeline_dbscan.fit(final_df)\n",
        "# pipeline_agglo.fit(final_df)\n",
        "\n",
        "# # Get cluster labels from each algorithm\n",
        "# labels_kmeans = pipeline_kmeans.named_steps['cluster'].labels_\n",
        "# labels_dbscan = pipeline_dbscan.named_steps['cluster'].labels_\n",
        "# labels_agglo = pipeline_agglo.named_steps['cluster'].labels_\n",
        "\n",
        "# # If DBSCAN results in a single cluster, we already changed parameters above\n",
        "# if len(set(labels_dbscan)) <= 1:\n",
        "#     print(\"DBSCAN resulted in a single cluster, parameters may still need optimization.\")\n",
        "\n",
        "# # Gather clustering results\n",
        "# clusterings = [labels_kmeans, labels_dbscan, labels_agglo]\n",
        "\n",
        "# # Create the co-association matrix\n",
        "# n_samples = final_df.shape[0]\n",
        "# co_assoc_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "# for labels in clusterings:\n",
        "#     for i in range(n_samples):\n",
        "#         for j in range(n_samples):\n",
        "#             if labels[i] == labels[j]:\n",
        "#                 co_assoc_matrix[i, j] += 1\n",
        "\n",
        "# # Normalize the co-association matrix\n",
        "# co_assoc_matrix /= len(clusterings)\n",
        "\n",
        "# # Perform clustering on the co-association matrix\n",
        "# distance_matrix = 1 - co_assoc_matrix\n",
        "# hierarchical_clustering = linkage(squareform(distance_matrix), method='average')\n",
        "# ensemble_labels = fcluster(hierarchical_clustering, t=8, criterion='maxclust')\n",
        "\n",
        "# # Add ensemble labels to the dataframe\n",
        "# final_df['ensemble_cluster_label'] = ensemble_labels\n",
        "\n",
        "# # Preprocess the data for evaluation metrics\n",
        "# preprocessed_data = preprocessor.fit_transform(final_df)\n",
        "\n",
        "# # Check the number of unique clusters in the ensemble result\n",
        "# unique_clusters = len(set(ensemble_labels))\n",
        "# if unique_clusters > 1:\n",
        "#     # Evaluate the ensemble clustering result\n",
        "#     silhouette_avg = silhouette_score(preprocessed_data, ensemble_labels)\n",
        "#     davies_bouldin_avg = davies_bouldin_score(preprocessed_data, ensemble_labels)\n",
        "#     print('Ensemble Silhouette Score:', silhouette_avg)\n",
        "#     print('Ensemble Davies-Bouldin Index:', davies_bouldin_avg)\n",
        "# else:\n",
        "#     print(f\"Ensemble clustering resulted in {unique_clusters} unique cluster(s). Silhouette score and Davies-Bouldin Index cannot be computed.\")\n",
        "\n",
        "# # Print the dataframe with cluster labels\n",
        "# print(final_df[['unit_id', 'ensemble_cluster_label']])\n"
      ],
      "metadata": {
        "id": "_-a4ncE4ikm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_cols = ['total_guest_capacity', 'avg_price_per_day', 'area_distance_to_airport']\n",
        "cat_cols = ['property_type', 'area_regency_city']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "encoder = OneHotEncoder(drop='first')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, num_cols),\n",
        "        ('cat', encoder, cat_cols)\n",
        "    ])\n",
        "\n",
        "cluster = KMeans(n_clusters=8, init='random', n_init=50, max_iter=300, tol=1e-04, random_state=0)\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('cluster', cluster)])\n",
        "\n",
        "pipeline.fit(final_df)\n",
        "cluster_labels = pipeline.named_steps['cluster'].labels_\n",
        "\n",
        "final_df['cluster_label'] = cluster_labels\n",
        "\n",
        "print(final_df[['unit_id', 'cluster_label']])"
      ],
      "metadata": {
        "id": "wSfMDph4cbvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Assuming `final_df` is your dataset with appropriate columns\n",
        "\n",
        "# Define the columns that need to be scaled and encoded\n",
        "num_cols = ['total_guest_capacity', 'avg_price_per_day', 'area_distance_to_airport']\n",
        "cat_cols = ['property_type', 'area_regency_city']\n",
        "\n",
        "# Create the transformers\n",
        "scaler = MinMaxScaler()\n",
        "encoder = OneHotEncoder(drop='first')\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, num_cols),\n",
        "        ('cat', encoder, cat_cols)\n",
        "    ])\n",
        "\n",
        "# Define the KMeans clustering algorithm with the new parameters\n",
        "kmeans = KMeans(n_clusters=8, init='random', n_init=50, max_iter=300, tol=1e-04, random_state=0)\n",
        "\n",
        "# Define the DBSCAN clustering algorithm\n",
        "dbscan = DBSCAN(eps=1.15, min_samples=3)\n",
        "\n",
        "# Create pipelines for each clustering algorithm\n",
        "pipeline_kmeans = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', kmeans)])\n",
        "pipeline_dbscan = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', dbscan)])\n",
        "\n",
        "# Fit the pipelines\n",
        "pipeline_kmeans.fit(final_df)\n",
        "pipeline_dbscan.fit(final_df)\n",
        "\n",
        "# Get cluster labels from each algorithm\n",
        "labels_kmeans = pipeline_kmeans.named_steps['cluster'].labels_\n",
        "labels_dbscan = pipeline_dbscan.named_steps['cluster'].labels_\n",
        "\n",
        "# If DBSCAN results in a single cluster, modify eps or min_samples\n",
        "if len(set(labels_dbscan)) <= 1:\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    pipeline_dbscan = Pipeline(steps=[('preprocessor', preprocessor), ('cluster', dbscan)])\n",
        "    pipeline_dbscan.fit(final_df)\n",
        "    labels_dbscan = pipeline_dbscan.named_steps['cluster'].labels_\n",
        "\n",
        "# Gather clustering results\n",
        "clusterings = [labels_kmeans, labels_dbscan]\n",
        "\n",
        "# Create the co-association matrix\n",
        "n_samples = final_df.shape[0]\n",
        "co_assoc_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "for labels in clusterings:\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                co_assoc_matrix[i, j] += 1\n",
        "\n",
        "# Normalize the co-association matrix\n",
        "co_assoc_matrix /= len(clusterings)\n",
        "\n",
        "# Perform clustering on the co-association matrix\n",
        "distance_matrix = 1 - co_assoc_matrix\n",
        "hierarchical_clustering = linkage(squareform(distance_matrix), method='average')\n",
        "ensemble_labels = fcluster(hierarchical_clustering, t=8, criterion='maxclust')\n",
        "\n",
        "# Add ensemble labels to the dataframe\n",
        "final_df['ensemble_cluster_label'] = ensemble_labels\n",
        "\n",
        "# Preprocess the data for evaluation metrics\n",
        "preprocessed_data = preprocessor.fit_transform(final_df)\n",
        "\n",
        "# Check the number of unique clusters in the ensemble result\n",
        "unique_clusters = len(set(ensemble_labels))\n",
        "if unique_clusters > 1:\n",
        "    # Evaluate the ensemble clustering result\n",
        "    silhouette_avg = silhouette_score(preprocessed_data, ensemble_labels)\n",
        "    davies_bouldin_avg = davies_bouldin_score(preprocessed_data, ensemble_labels)\n",
        "    print('Ensemble Silhouette Score:', silhouette_avg)\n",
        "    print('Ensemble Davies-Bouldin Index:', davies_bouldin_avg)\n",
        "else:\n",
        "    print(f\"Ensemble clustering resulted in {unique_clusters} unique cluster(s). Silhouette score and Davies-Bouldin Index cannot be computed.\")\n",
        "\n",
        "# Print the dataframe with cluster labels\n",
        "print(final_df[['unit_id', 'ensemble_cluster_label']])\n"
      ],
      "metadata": {
        "id": "jUx-NY9EeQ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_avg = silhouette_score(preprocessed_data, ensemble_labels)\n",
        "davies_bouldin_avg = davies_bouldin_score(preprocessed_data, ensemble_labels)\n",
        "print('Ensemble Silhouette Score:', silhouette_avg)\n",
        "print('Ensemble Davies-Bouldin Index:', davies_bouldin_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfI3-5J10aHu",
        "outputId": "477ee6c6-60df-4e9b-a6c3-95c4ea74da76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Silhouette Score: 0.782397197411991\n",
            "Ensemble Davies-Bouldin Index: 0.8882339537840735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7gIkBW009rH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}